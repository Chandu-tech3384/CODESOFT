{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3322096,"sourceType":"datasetVersion","datasetId":2008274}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# 1. Simulate Dataset\nnp.random.seed(42)\nn_samples = 500\n\ndata = {\n    'age': np.random.randint(18, 70, size=n_samples),\n    'gender': np.random.choice(['Male', 'Female'], size=n_samples),\n    'tenure': np.random.randint(1, 60, size=n_samples),  # months\n    'monthly_usage': np.random.normal(20, 5, size=n_samples).round(2),  # hours\n    'contract_type': np.random.choice(['Monthly', 'Yearly'], size=n_samples),\n    'payment_method': np.random.choice(['Credit Card', 'PayPal', 'Bank Transfer'], size=n_samples),\n    'churn': np.random.choice([0, 1], size=n_samples, p=[0.75, 0.25])  # 25% churn rate\n}\n\ndf = pd.DataFrame(data)\n\n# 2. Preprocessing\nle_gender = LabelEncoder()\nle_contract = LabelEncoder()\nle_payment = LabelEncoder()\n\ndf['gender'] = le_gender.fit_transform(df['gender'])\ndf['contract_type'] = le_contract.fit_transform(df['contract_type'])\ndf['payment_method'] = le_payment.fit_transform(df['payment_method'])\n\n# 3. Features & Target\nX = df.drop('churn', axis=1)\ny = df['churn']\n\n# 4. Split Data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 5. Scale Features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 6. Train Models\n\n# Logistic Regression\nlr = LogisticRegression()\nlr.fit(X_train_scaled, y_train)\ny_pred_lr = lr.predict(X_test_scaled)\n\n# Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)  # Tree models don't need scaling\ny_pred_rf = rf.predict(X_test)\n\n# Gradient Boosting\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\ngb.fit(X_train, y_train)\ny_pred_gb = gb.predict(X_test)\n\n# 7. Evaluation\nprint(\"=== Logistic Regression ===\")\nprint(classification_report(y_test, y_pred_lr))\n\nprint(\"=== Random Forest ===\")\nprint(classification_report(y_test, y_pred_rf))\n\nprint(\"=== Gradient Boosting ===\")\nprint(classification_report(y_test, y_pred_gb))\n\n# 8. Accuracy Comparison\nprint(\"Accuracy Scores:\")\nprint(\"Logistic Regression:\", accuracy_score(y_test, y_pred_lr))\nprint(\"Random Forest:\", accuracy_score(y_test, y_pred_rf))\nprint(\"Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T06:18:57.844609Z","iopub.execute_input":"2025-06-03T06:18:57.844862Z","iopub.status.idle":"2025-06-03T06:18:59.850762Z","shell.execute_reply.started":"2025-06-03T06:18:57.844840Z","shell.execute_reply":"2025-06-03T06:18:59.849723Z"}},"outputs":[{"name":"stdout","text":"=== Logistic Regression ===\n              precision    recall  f1-score   support\n\n           0       0.74      1.00      0.85        74\n           1       0.00      0.00      0.00        26\n\n    accuracy                           0.74       100\n   macro avg       0.37      0.50      0.43       100\nweighted avg       0.55      0.74      0.63       100\n\n=== Random Forest ===\n              precision    recall  f1-score   support\n\n           0       0.76      0.97      0.85        74\n           1       0.60      0.12      0.19        26\n\n    accuracy                           0.75       100\n   macro avg       0.68      0.54      0.52       100\nweighted avg       0.72      0.75      0.68       100\n\n=== Gradient Boosting ===\n              precision    recall  f1-score   support\n\n           0       0.76      0.97      0.85        74\n           1       0.60      0.12      0.19        26\n\n    accuracy                           0.75       100\n   macro avg       0.68      0.54      0.52       100\nweighted avg       0.72      0.75      0.68       100\n\nAccuracy Scores:\nLogistic Regression: 0.74\nRandom Forest: 0.75\nGradient Boosting: 0.75\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":1}]}